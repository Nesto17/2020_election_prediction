glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid),
# rf = caretModelSpec(method = "ranger", tuneGrid = rfGrid),
enet = caretModelSpec(method = "enet", tuneGrid = enetGrid),
gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
)
)
stacked_model
final_model <- caretEnsemble(stacked_model)
pred <- final_model %>% predict(newdata = y)
pred <- test %>%
select(id) %>%
bind_cols(pred)
pred <- final_model %>% predict(newdata = y)
pred <- test %>%
select(id) %>%
bind_cols(pred)
pred
pred <- final_model %>% predict(newdata = y)
pred <- test %>%
select(id) %>%
bind_cols(pred)
pred <- final_model %>% predict(newdata = y)
pred
tibble(pred)
pred <- test %>%
select(id) %>%
bind_cols(tibble(pred))
pred
write_csv(pred, "pred.csv")
save.image("~/University Works/Summer 2023/STATS 101C/Final Projects/Regression/caret.RData")
svmFit <- train(percent_dem ~ ., data = x,
method = "svmRadial",
trControl = fitControl,
tuneLength = 8,
metric = "rmse")
svmFit <- train(percent_dem ~ ., data = x,
method = "svmRadial",
trControl = trControl,
tuneLength = 8,
metric = "rmse")
trControl <- trainControl(
method = "cv",
savePredictions = "final",
index = createMultiFolds(x$percent_dem, k = 10, times = 3),
allowParallel = TRUE,
verboseIter = TRUE
)
# xgbTreeGrid <- expand.grid(
#   max_depth = 12,
#   nrounds = 888,
#
# )
xgbTreeGrid <- expand.grid(nrounds = 2000, max_depth = 4, eta = 0.02,
gamma = 0, colsample_bytree = 0.625, subsample = 0.5, min_child_weight = 4)
svmGrid <- expand.grid(sigma = 0.005, C = 40)
glmnetGrid <- expand.grid(alpha = 0.9, lambda = 0.01)
# rfGrid <- expand.grid(mtry = 64, splitrule = "variance", min.node.size = 2)
enetGrid <- expand.grid(fraction = 0.9, lambda = 0.01)
gauss_Grid <- expand.grid(sigma = 0.005)
set.seed(42)
stacked_model <- caretList(
percent_dem ~ ., data = x,
trControl = trControl,
metric = "RMSE",
tuneList = list(
xgb = caretModelSpec(method = "xgbTree", tuneGrid = xgbTreeGrid),
svm = caretModelSpec(method = "svmRadial", tuneGrid = svmGrid),
glmnet = caretModelSpec(method = "glmnet", tuneGrid = glmnetGrid),
# rf = caretModelSpec(method = "ranger", tuneGrid = rfGrid),
enet = caretModelSpec(method = "enet", tuneGrid = enetGrid),
gauss = caretModelSpec(method = "gaussprRadial", tuneGrid = gauss_Grid)
)
)
final_model <- caretEnsemble(stacked_model)
?caretList
xyplot(resamples(stacked_model))
pred <- final_model %>% predict(newdata = y)
pred <- test %>%
select(id) %>%
bind_cols(tibble(pred))
write_csv(pred, "pred2.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(tidyverse)
library(car)
library(parsnip)
library(recipes)
library(yardstick)
library(dials)
library(tune)
library(workflowsets)
library(xgboost)
library(lightgbm)
library(glmnet)
library(kernlab)
library(ranger)
library(dbarts)
library(bonsai)
library(mgcv)
library(LiblineaR)
library(Cubist)
library(rules)
train <- read_csv("train_25.csv")
test <- read_csv("test.csv")
# V-Folds CV
set.seed(2023)
folds <- vfold_cv(train, v = 10, strata = percent_dem)
# Preprocessing
rec <- recipe(percent_dem ~ ., data = train) %>%
step_impute_mean(all_numeric_predictors()) %>%
step_mutate(x2013_code = factor(x2013_code)) %>%
step_corr(all_numeric_predictors(), threshold = 0.999) %>%
step_nzv(all_numeric_predictors()) %>%
step_log(all_numeric_predictors(), offset = 1) %>%
step_normalize(all_numeric_predictors()) %>%
step_dummy(all_nominal())
# Models Spec
# Model 1 - XGBoost
xgboost_spec <- boost_tree(
trees = 888,
min_n = 39,
tree_depth = 12,
learn_rate = 0.09
) %>%
set_engine("xgboost") %>%
set_mode("regression")
# Model 2 - Light GBM
lightgbm_spec <- parsnip::boost_tree(
trees = 1000,
tree_depth = 13,
min_n = 20
) %>%
set_engine("lightgbm") %>%
set_mode("regression")
# Model 3 - BART
bart_spec <- parsnip::bart(
trees = 333,
prior_terminal_node_coef = 0.978,
prior_terminal_node_expo = 2.28
) %>%
set_engine("dbarts") %>%
set_mode("regression")
# Model 4 - Random Forest
randforest_spec <- parsnip::rand_forest(
trees = 1510,
min_n = 2
) %>%
set_engine("ranger") %>%
set_mode("regression")
# Model 5 - GLM Net
glmnet_spec <- parsnip::linear_reg(
penalty = 0.01,
mixture = 0.9
) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Model 6 - SVM with Radial Kernel
svmrbf_spec <- parsnip::svm_rbf(
cost = 40,
rbf_sigma = 0.005,
margin = 0.015
) %>%
set_engine("kernlab") %>%
set_mode("regression")
# Model 7 - Linear SVM
svmlin_spec <- parsnip::svm_linear(
cost = 5,
margin = 0.14
) %>%
set_engine("LiblineaR") %>%
set_mode("regression")
# Model 8 - Cubist rule-based Regression
cubist_spec <- parsnip::cubist_rules(
committees = 83,
neighbors = 0,
max_rules = 101
) %>%
set_engine("Cubist") %>%
set_mode("regression")
wfl <- workflow_set(
preproc = list(mod = rec),
models = list(xgboost = xgboost_spec,
lightgbm = lightgbm_spec,
bart = bart_spec,
randforest = randforest_spec,
glmnet = glmnet_spec,
svmrbf = svmrbf_spec,
svmlin = svmlin_spec,
cubist = cubist_spec),
cross = TRUE
) %>% option_add(control = control_grid(save_workflow = TRUE,
save_pred = TRUE, verbose = TRUE))
wfl
set.seed(2023)
wfl_tuned <- wfl %>% workflow_map("fit_resamples", resamples = folds,
metrics = metric_set(rmse), verbose = TRUE)
wfl_tuned %>% collect_metrics() %>% arrange(mean)
wfl_tuned %>% workflowsets::autoplot()
rank_results(wfl_tuned, rank_metric = "rmse", select_best = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(pls)
library(tidyverse)
library(tidymodels)
library(workflowsets)
train <- read_csv("train.csv")
test <- read_csv("test.csv")
# Removing unnecessary columns
train <- train %>% select(-c(id, name)) # Name is not a predictor.
# Removing features which contain the amount of zeroes > 25% of the whole dataset
treshold <- 0.25
cols_to_remove <- colnames(train)[colSums(train == 0) / nrow(train) > treshold]
cols_to_remove <- cols_to_remove[!is.na(cols_to_remove)]
train <- train[, !(names(train) %in% cols_to_remove)]
train %>% dim()
train25 <- read_csv("train_25.csv")
train25 %>% dim()
train <- read_csv("train.csv")
test <- read_csv("test.csv")
# Removing unnecessary columns
train <- train %>% select(-c(id, name)) # Name is not a predictor.
# Removing features which contain the amount of zeroes > 25% of the whole dataset
treshold <- 0.5
cols_to_remove <- colnames(train)[colSums(train == 0) / nrow(train) > treshold]
cols_to_remove <- cols_to_remove[!is.na(cols_to_remove)]
train <- train[, !(names(train) %in% cols_to_remove)]
train25 %>% dim()
train %>% dim()
train25 <- read_csv("train_50.csv")
train25 %>% dim()
train %>% dim()
train <- read_csv("train_25.csv")
test <- read_csv("test.csv")
?add_candidates
load("~/University Works/Summer 2023/STATS 101C/Final Projects/Regression/stack-3.RData")
knitr::opts_chunk$set(echo = TRUE)
# Model Stack
set.seed(2023)
model_stack <- data_stack %>%
blend_predictions()
library(tidymodels)
library(stacks)
library(tune)
library(rsample)
library(parsnip)
library(workflows)
library(recipes)
library(yardstick)
library(tidyverse)
library(dplyr)
library(purrr)
library(ggplot2)
library(xgboost)
library(lightgbm)
library(glmnet)
library(kernlab)
library(ranger)
library(dbarts)
library(bonsai)
library(mgcv)
library(LiblineaR)
library(Cubist)
library(rules)
# Model Stack
set.seed(2023)
model_stack <- data_stack %>%
blend_predictions()
model_stack %>% autoplot()
model_stack %>% autoplot(type = "weights")
collect_parameters(final_stack, "xgboost")
collect_parameters(final_stack, "lightgbm")
collect_parameters(final_stack, "bart")
collect_parameters(final_stack, "randforest")
collect_parameters(final_stack, "glmnet")
collect_parameters(final_stack, "svm_rbf")
collect_parameters(final_stack, "svm_lin")
collect_parameters(final_stack, "cubist")
collect_parameters(final_stack, "xgboost")
collect_parameters(final_stack, "lightgbm")
collect_parameters(final_stack, "bart")
collect_parameters(final_stack, "randforest")
collect_parameters(final_stack, "glmnet")
collect_parameters(final_stack, "svm_rbf")
collect_parameters(final_stack, "svm_lin")
collect_parameters(final_stack, "cubist")
collect_parameters(model_stack, "xgboost")
collect_parameters(final_stack, "lightgbm")
collect_parameters(final_stack, "bart")
collect_parameters(final_stack, "randforest")
collect_parameters(final_stack, "glmnet")
collect_parameters(final_stack, "svm_rbf")
collect_parameters(final_stack, "svm_lin")
collect_parameters(final_stack, "cubist")
collect_parameters(model_stack, "xgboost")
collect_parameters(model_stack, "lightgbm")
collect_parameters(model_stack, "bart")
collect_parameters(model_stack, "randforest")
collect_parameters(model_stack, "glmnet")
collect_parameters(model_stack, "svm_rbf")
collect_parameters(model_stack, "svm_lin")
collect_parameters(model_stack, "cubist")
source("~/University Works/Summer 2023/STATS 101C/Final Projects/Regression/script.R", echo=TRUE)
# Removing features which contain the amount of zeroes > 25% of the whole dataset
treshold <- 0.25
cols_to_remove <- colnames(train)[colSums(train == 0) / nrow(train) > treshold]
# Reading the datasets
train <- read_csv("train.csv")
# Load required libraries
library(tidyverse)
library(tidymodels)
library(stacks)
library(xgboost)
library(lightgbm)
library(bonsai)
library(dbarts)
library(ranger)
library(glmnet)
library(kernlab)
library(LinlineaR)
# Load required libraries
library(tidyverse)
library(tidymodels)
library(stacks)
library(xgboost)
library(lightgbm)
library(bonsai)
library(dbarts)
library(ranger)
library(glmnet)
library(kernlab)
library(LiblineaR)
library(Cubist)
library(rules)
# Reading the datasets
train <- read_csv("train.csv")
# Load required libraries
library(tidyverse)
library(tidymodels)
library(stacks)
library(xgboost)
library(lightgbm)
library(bonsai)
library(dbarts)
library(ranger)
library(glmnet)
library(kernlab)
library(LiblineaR)
library(Cubist)
library(rules)
# Reading the datasets
train <- read_csv("train.csv")
setwd("~/University Works/Summer 2023/STATS 101C/Final Projects/Regression")
# Load required libraries
library(tidyverse)
library(tidymodels)
library(stacks)
library(xgboost)
library(lightgbm)
library(bonsai)
library(dbarts)
library(ranger)
library(glmnet)
library(kernlab)
library(LiblineaR)
library(Cubist)
library(rules)
# Reading the datasets
train <- read_csv("train.csv")
test <- read_csv("test.csv")
# Removing unnecessary columns
train <- train %>% select(-c(id, name)) # Name is not a predictor.
# Removing features which contain the amount of zeroes > 25% of the whole dataset
treshold <- 0.25
cols_to_remove <- colnames(train)[colSums(train == 0) / nrow(train) > treshold]
cols_to_remove <- cols_to_remove[!is.na(cols_to_remove)]
train <- train[, !(names(train) %in% cols_to_remove)]
# ------------------------------------------------------------
# Creating 10 cross-validation folds
set.seed(2020)
folds <- vfold_cv(train, v = 10, strata = percent_dem)
# Preprocessing recipe
rec <- recipe(percent_dem ~ ., data = train) %>%
step_impute_mean(all_numeric_predictors()) %>% # Impute NAs with mean
step_mutate(x2013_code = factor(x2013_code)) %>% # Mutate categorical features
step_corr(all_numeric_predictors(), threshold = 0.999) %>% # Remove predictors that are extremely correlated
step_nzv(all_numeric_predictors()) %>% # Remove predictors that are highly sparse
step_log(all_numeric_predictors(), offset = 1) %>% # Log transform numerical predictors
step_normalize(all_numeric_predictors()) %>% # Scale numerical predictors
step_dummy(all_nominal()) # Encode categorical features
# Define metric set
metric <- metric_set(rmse)
# ------------------------------------------------------------
# Model specifications
# Model 1 - XGBoost
xgboost_spec <- boost_tree(
trees = 888,
min_n = 39,
tree_depth = 12,
learn_rate = 0.09
) %>%
set_engine("xgboost") %>%
set_mode("regression")
# Model 2 - Light GBM
lightgbm_spec <- parsnip::boost_tree(
trees = 1000,
tree_depth = 13,
min_n = 20
) %>%
set_engine("lightgbm") %>%
set_mode("regression")
# Model 3 - BART
bart_spec <- parsnip::bart(
trees = 333,
prior_terminal_node_coef = 0.978,
prior_terminal_node_expo = 2.28
) %>%
set_engine("dbarts") %>%
set_mode("regression")
# Model 4 - Random Forest
randforest_spec <- parsnip::rand_forest(
trees = 1510,
min_n = 2
) %>%
set_engine("ranger") %>%
set_mode("regression")
# Model 5 - GLM Net
glmnet_spec <- parsnip::linear_reg(
penalty = 0.01,
mixture = 0.9
) %>%
set_engine("glmnet") %>%
set_mode("regression")
# Model 6 - SVM with Radial Kernel
svmrbf_spec <- parsnip::svm_rbf(
cost = 40,
rbf_sigma = 0.005,
margin = 0.015
) %>%
set_engine("kernlab") %>%
set_mode("regression")
# Model 7 - Linear SVM
svmlin_spec <- parsnip::svm_linear(
cost = 5,
margin = 0.14
) %>%
set_engine("LiblineaR") %>%
set_mode("regression")
# Model 8 - Cubist rule-based Regression
cubist_spec <- parsnip::cubist_rules(
committees = 83,
neighbors = 0,
max_rules = 101
) %>%
set_engine("Cubist") %>%
set_mode("regression")
model_specs <- list(xgboost_spec, lightgbm_spec, bart_spec, randforest_spec,
glmnet_spec, svmrbf_spec, svmlin_spec, cubist_spec)
names(model_specs) <- c("xgboost", "lightgbm", "bart", "randforest",
"glmnet", "svmrbf", "svmlin", "cubist")
# ------------------------------------------------------------
# Individually create workflows
wfls <- list()
for (spec in names(model_specs)) {
wfl <- workflow() %>%
add_model(model_specs[[spec]]) %>%
add_recipe(rec)
wfl_name <- paste(c(spec, "_wfl"), collapse = "")
wfls[[wfl_name]] <- wfl
}
# Fit validation folds to each workflow
results <- list()
for (wfl in names(wfls)) {
res <- wfls[[wfl]] %>% fit_resamples(
resamples = folds,
metrics = metric,
control = control_stack_resamples()
)
res_name <- paste(c(wfl, "res"), collapse = "")
results[[res_name]] <- res
}
# ------------------------------------------------------------
# Constructing data stack
xgboost <- results$xgboost_wflres
lightgbm <- results$lightgbm_wflres
bart <- results$bart_wflres
randforest <- results$randforest_wflres
glmnet <- results$glmnet_wflres
svm_rbf <- results$svmrbf_wflres
svm_lin <- results$svmlin_wflres
cubist <- results$cubist_wflres
data_stack <- stacks() %>%
add_candidates(xgboost) %>%
add_candidates(lightgbm) %>%
add_candidates(bart) %>%
add_candidates(randforest) %>%
add_candidates(glmnet) %>%
add_candidates(svm_rbf) %>%
add_candidates(svm_lin) %>%
add_candidates(cubist)
# Constructing final model stack with non-zero stacking coefficients
set.seed(2023)
model_stack <- data_stack %>%
blend_predictions() %>%
fit_members()
# ------------------------------------------------------------
# Making predictions
pred <- model_stack %>%
predict(test)
# Creating prediction tibble and export
pred <- test %>%
select(id) %>%
bind_cols(pred)
write_csv(pred, "pred.csv")
